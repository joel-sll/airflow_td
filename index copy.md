Voici une version dÃ©taillÃ©e des Ã©tapes pour chaque exercice, en respectant la structure **Exercice / Ressources / Astuce / Solution**. J'ai ajoutÃ© deux nouveaux exercices : un pour surveiller le fichier CSV avec un **Sensor** et un autre pour envoyer une notification Slack lorsque le pipeline est terminÃ©.

---

# ğŸš€ Ã‰tape 3 : CrÃ©ation d'un pipeline ETL avec Airflow

## ğŸ¯ Objectif

CrÃ©er un pipeline ETL (Extract, Transform, Load) avec Airflow pour :

1. Extraire des donnÃ©es de ventes de magasins en France et aux Ã‰tats-Unis.
2. Transformer les donnÃ©es en calculant le total des ventes.
3. Charger les donnÃ©es transformÃ©es dans un fichier CSV.
4. Surveiller les modifications du fichier CSV avec un **Sensor**.
5. Envoyer une notification Slack lorsque le pipeline est terminÃ©.

---

## ğŸ“š Ressources
- [Documentation officielle d'Airflow](https://airflow.apache.org/docs/)
- [PythonOperator](https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/python.html)
- [FileSensor](https://airflow.apache.org/docs/apache-airflow/stable/howto/sensor.html)
- [SlackOperator](https://airflow.apache.org/docs/apache-airflow-providers-slack/stable/operators/slack.html)
- [Pandas Documentation](https://pandas.pydata.org/docs/)

---

## ğŸ“ Ã‰tapes du TD

### Exercice 1 : Mise en place du pipeline de la France et des USA

#### Partie 1 : Extraction des donnÃ©es

**Objectif** : Extraire des donnÃ©es de ventes pour la France et les USA.

??? tip "Astuce"
    - Utilisez des fonctions simples pour simuler l'extraction des donnÃ©es.
    - Stockez les donnÃ©es dans XCom pour les rÃ©utiliser dans les tÃ¢ches suivantes.

??? example "Code initial"
    ```python
    def extract_france():
        """Simule l'extraction des donnÃ©es de ventes en France."""
        return {"ventes": [100, 200, 300]}  # Exemple de donnÃ©es

    def extract_usa():
        """Simule l'extraction des donnÃ©es de ventes aux USA."""
        return {"ventes": [500, 600, 700]}  # Exemple de donnÃ©es
    ```

**Ã‰tapes** :

1. CrÃ©ez un fichier `etl_ventes_dag.py` dans le dossier `dags` d'Airflow.

2. DÃ©finissez les fonctions `extract_france` et `extract_usa` pour simuler l'extraction des donnÃ©es.

3. Ajoutez les tÃ¢ches `extract_france_task` et `extract_usa_task` dans le DAG.

??? success "Solution complÃ¨te"
    ```python
    from airflow import DAG
    from airflow.operators.python_operator import PythonOperator
    from datetime import datetime, timedelta

    # Fonctions d'extraction
    def extract_france():
        return {"ventes": [100, 200, 300]}

    def extract_usa():
        return {"ventes": [500, 600, 700]}

    # Configuration du DAG
    default_args = {
        'owner': 'votre_nom',
        'depends_on_past': False,
        'start_date': datetime(2023, 10, 1),
        'retries': 2,
        'retry_delay': timedelta(minutes=10),
    }

    dag = DAG(
        'etl_ventes_pipeline',
        default_args=default_args,
        description='Pipeline ETL pour les donnÃ©es de vente',
        schedule_interval=timedelta(minutes=5),
        catchup=False,
    )

    # TÃ¢ches d'extraction
    extract_france_task = PythonOperator(
        task_id='extract_france',
        python_callable=extract_france,
        dag=dag,
    )

    extract_usa_task = PythonOperator(
        task_id='extract_usa',
        python_callable=extract_usa,
        dag=dag,
    )
    ```

---

#### Partie 2 : Transformation des donnÃ©es

**Objectif** : Transformer les donnÃ©es extraites en calculant le total des ventes.

??? tip "Astuce"
    - Utilisez `xcom_pull` pour rÃ©cupÃ©rer les donnÃ©es des tÃ¢ches d'extraction.
    - Appliquez des transformations simples comme la somme des ventes.

??? example "Code initial"
    ```python
    def transform_france(**context):
        """Transformation des donnÃ©es France."""
        ventes_france = context['ti'].xcom_pull(task_ids='extract_france')
        return {"region": "France", "total_ventes": sum(ventes_france["ventes"])}

    def transform_usa(**context):
        """Transformation des donnÃ©es USA."""
        ventes_usa = context['ti'].xcom_pull(task_ids='extract_usa')
        return {"region": "USA", "total_ventes": sum(ventes_usa["ventes"])}
    ```

**Ã‰tapes** :

1. DÃ©finissez les fonctions `transform_france` et `transform_usa` pour transformer les donnÃ©es.

2. Ajoutez les tÃ¢ches `transform_france_task` et `transform_usa_task` dans le DAG.

3. DÃ©finissez les dÃ©pendances entre les tÃ¢ches d'extraction et de transformation.

??? success "Solution complÃ¨te"
    ```python
    # Fonctions de transformation
    def transform_france(**context):
        ventes_france = context['ti'].xcom_pull(task_ids='extract_france')
        return {"region": "France", "total_ventes": sum(ventes_france["ventes"])}

    def transform_usa(**context):
        ventes_usa = context['ti'].xcom_pull(task_ids='extract_usa')
        return {"region": "USA", "total_ventes": sum(ventes_usa["ventes"])}

    # TÃ¢ches de transformation
    transform_france_task = PythonOperator(
        task_id='transform_france',
        python_callable=transform_france,
        provide_context=True,
        dag=dag,
    )

    transform_usa_task = PythonOperator(
        task_id='transform_usa',
        python_callable=transform_usa,
        provide_context=True,
        dag=dag,
    )

    # DÃ©finition du flux
    extract_france_task >> transform_france_task
    extract_usa_task >> transform_usa_task
    ```

---

#### Partie 3 : Chargement des donnÃ©es dans un CSV

**Objectif** : Charger les donnÃ©es transformÃ©es dans un fichier CSV.

??? tip "Astuce"
    - Utilisez `pandas` pour crÃ©er un DataFrame et sauvegarder les donnÃ©es dans un fichier CSV.
    - Assurez-vous que le rÃ©pertoire de sortie existe.

??? example "Code initial"
    ```python
    import pandas as pd

    def load_data(**context):
        """Charge les donnÃ©es transformÃ©es dans un fichier CSV."""
        data_france = context['ti'].xcom_pull(task_ids='transform_france')
        data_usa = context['ti'].xcom_pull(task_ids='transform_usa')
        df = pd.DataFrame([data_france, data_usa])
        df.to_csv('output/ventes.csv', index=False)
    ```

**Ã‰tapes** :

1. DÃ©finissez la fonction `load_data` pour charger les donnÃ©es dans un fichier CSV.

2. Ajoutez la tÃ¢che `load_data_task` dans le DAG.

3. DÃ©finissez les dÃ©pendances entre les tÃ¢ches de transformation et de chargement.

??? success "Solution complÃ¨te"
    ```python
    import os
    import pandas as pd

    def load_data(**context):
        """Charge les donnÃ©es transformÃ©es dans un fichier CSV."""
        data_france = context['ti'].xcom_pull(task_ids='transform_france')
        data_usa = context['ti'].xcom_pull(task_ids='transform_usa')
        df = pd.DataFrame([data_france, data_usa])
        os.makedirs('output', exist_ok=True)
        df.to_csv('output/ventes.csv', index=False)

    # TÃ¢che de chargement
    load_data_task = PythonOperator(
        task_id='load_data',
        python_callable=load_data,
        provide_context=True,
        dag=dag,
    )

    # DÃ©finition du flux
    [transform_france_task, transform_usa_task] >> load_data_task
    ```

---

### Exercice 2 : Surveillance du fichier CSV avec un Sensor

**Objectif** : Surveiller les modifications du fichier CSV avec un **Sensor**.

??? tip "Astuce"
    - Utilisez `FileSensor` pour surveiller les modifications du fichier CSV.
    - Configurez le `filepath` pour pointer vers le fichier CSV.

??? example "Code initial"
    ```python
    from airflow.sensors.filesystem import FileSensor

    file_sensor_task = FileSensor(
        task_id='file_sensor',
        filepath='output/ventes.csv',
        poke_interval=30,  # VÃ©rifie toutes les 30 secondes
        timeout=300,       # Timeout aprÃ¨s 5 minutes
        mode='poke',
    )
    ```

**Ã‰tapes** :

1. Ajoutez la tÃ¢che `file_sensor_task` pour surveiller le fichier CSV.

2. DÃ©finissez les dÃ©pendances entre la tÃ¢che de chargement et le **Sensor**.

??? success "Solution complÃ¨te"
    ```python
    from airflow.sensors.filesystem import FileSensor

    # TÃ¢che de surveillance
    file_sensor_task = FileSensor(
        task_id='file_sensor',
        filepath='output/ventes.csv',
        poke_interval=30,
        timeout=300,
        mode='poke',
        dag=dag,
    )

    # DÃ©finition du flux
    load_data_task >> file_sensor_task
    ```

---

### Exercice 3 : Envoi d'une notification Slack

**Objectif** : Envoyer une notification Slack lorsque le pipeline est terminÃ©.

??? tip "Astuce"
    - Utilisez `SlackOperator` pour envoyer une notification Slack.
    - Configurez les paramÃ¨tres `token`, `channel`, et `message` pour la notification.

??? example "Code initial"
    ```python
    from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator

    slack_task = SlackWebhookOperator(
        task_id='send_slack_notification',
        slack_webhook_conn_id='slack_default',
        message="Le pipeline ETL est terminÃ© avec succÃ¨s !",
    )
    ```

**Ã‰tapes** :

1. Configurez la connexion Slack dans Airflow (via l'interface web ou en utilisant une connexion par dÃ©faut).

2. Ajoutez la tÃ¢che `slack_task` pour envoyer une notification Slack.

3. DÃ©finissez les dÃ©pendances entre le **Sensor** et la tÃ¢che de notification Slack.

??? success "Solution complÃ¨te"
    ```python
    from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator

    # TÃ¢che de notification Slack
    slack_task = SlackWebhookOperator(
        task_id='send_slack_notification',
        slack_webhook_conn_id='slack_default',
        message="Le pipeline ETL est terminÃ© avec succÃ¨s !",
        dag=dag,
    )

    # DÃ©finition du flux
    file_sensor_task >> slack_task
    ```

---

### Exercice 4 : GÃ©nÃ©ration d'un rapport consolidÃ©

**Objectif** : GÃ©nÃ©rer un rapport consolidÃ© Ã  partir des donnÃ©es transformÃ©es.

??? tip "Astuce"
    - Utilisez `pandas` pour gÃ©nÃ©rer un rapport consolidÃ©.
    - Ajoutez des calculs supplÃ©mentaires comme la moyenne ou le total des ventes.

??? example "Code initial"
    ```python
    def generate_report(**context):
        """GÃ©nÃ¨re un rapport consolidÃ©."""
        df = pd.read_csv('output/ventes.csv')
        total_ventes = df['total_ventes'].sum()
        moyenne_ventes = df['total_ventes'].mean()
        rapport = f"Total des ventes : {total_ventes}\nMoyenne des ventes : {moyenne_ventes}"
        return rapport
    ```

**Ã‰tapes** :

1. DÃ©finissez la fonction `generate_report` pour gÃ©nÃ©rer un rapport consolidÃ©.

2. Ajoutez la tÃ¢che `generate_report_task` dans le DAG.

3. DÃ©finissez les dÃ©pendances entre le **Sensor** et la tÃ¢che de gÃ©nÃ©ration de rapport.

??? success "Solution complÃ¨te"
    ```python
    def generate_report(**context):
        """GÃ©nÃ¨re un rapport consolidÃ©."""
        df = pd.read_csv('output/ventes.csv')
        total_ventes = df['total_ventes'].sum()
        moyenne_ventes = df['total_ventes'].mean()
        rapport = f"Total des ventes : {total_ventes}\nMoyenne des ventes : {moyenne_ventes}"
        return rapport

    # TÃ¢che de gÃ©nÃ©ration de rapport
    generate_report_task = PythonOperator(
        task_id='generate_report',
        python_callable=generate_report,
        provide_context=True,
        dag=dag,
    )

    # DÃ©finition du flux
    file_sensor_task >> generate_report_task
    ```

---

## ğŸ‰ RÃ©sultat final

Vous avez maintenant un pipeline ETL complet avec Airflow qui :
1. Extrait les donnÃ©es de ventes pour la France et les USA.
2. Transforme les donnÃ©es en calculant le total des ventes.
3. Charge les donnÃ©es transformÃ©es dans un fichier CSV.
4. Surveille les modifications du fichier CSV avec un **Sensor**.
5. Envoie une notification Slack lorsque le pipeline est terminÃ©.
6. GÃ©nÃ¨re un rapport consolidÃ©.

N'hÃ©sitez pas Ã  adapter ce pipeline Ã  vos besoins spÃ©cifiques ! ğŸ˜Š