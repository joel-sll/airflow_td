# üöÄ √âtape 3 : Cr√©ation d'un Pipeline MLOps avec Airflow

## **Objectif**
L'objectif de cet exercice est de mettre en place un **pipeline MLOps automatis√©** avec **Apache Airflow** et **Docker** pour g√©rer l'entra√Ænement et l'√©valuation d'un mod√®le de classification sur le dataset **Iris**.

Nous allons :
1. **Charger et pr√©traiter les donn√©es** üìä
2. **Entra√Æner un mod√®le RandomForest** üéØ
3. **√âvaluer la performance du mod√®le** üìà
4. **Mettre en place un DAG Airflow pour automatiser ces √©tapes**

---

## üìÇ **Structure du projet**

```plaintext
mlops/
‚îÇ‚îÄ‚îÄ airflow-docker/
‚îÇ   ‚îú‚îÄ‚îÄ dags/                      # Contient les DAGs (workflow Airflow)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mlops_pipeline.py
‚îÇ   ‚îÇ   ‚îÇ‚îÄ‚îÄ src/                            # Code m√©tier (pr√©traitement, entra√Ænement...)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py
‚îÇ   ‚îÇ   ‚îÇ‚îÄ‚îÄ data/                           # Datasets si besoin
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data.csv
‚îÇ   ‚îÇ   ‚îÇ‚îÄ‚îÄ models/                         # Stockage des mod√®les entra√Æn√©s
‚îÇ   ‚îú‚îÄ‚îÄ plugins/                   # Plugins Airflow si n√©cessaire
‚îÇ   ‚îú‚îÄ‚îÄ logs/                       # Logs d'ex√©cution
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml          # D√©ploiement Docker
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt            # D√©pendances

```

---

## **1Ô∏è‚É£ √âtape 1 - Pr√©traitement des donn√©es**
**üìå Objectif :** Charger et nettoyer les donn√©es Iris, puis les sauvegarder.

üìç **Fichier : `src/preprocessing.py`**
```python
import os
import pandas as pd
from sklearn.datasets import load_iris

def preprocess_data():
    print("Pr√©traitement des donn√©es...")

    data_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../data"))
    os.makedirs(data_dir, exist_ok=True)

    # Charger le dataset Iris
    iris = load_iris()
    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
    df['target'] = iris.target

    file_path = os.path.join(data_dir, "iris.csv")

    df.to_csv(file_path, index=False)
    print(f"Donn√©es sauvegard√©es : {file_path}")

    return file_path

```

---

## **2Ô∏è‚É£ √âtape 2 - Entra√Ænement du mod√®le**
**üìå Objectif :** Entra√Æner un mod√®le `RandomForestClassifier` et sauvegarder le mod√®le.

üìç **Fichier : `src/training.py`**
```python
import os
import pandas as pd
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

def train_model():
    print("Entra√Ænement du mod√®le...")

    data= os.path.abspath(os.path.join(os.path.dirname(__file__), "../data/iris.csv"))

    if not os.path.exists(data):
        raise FileNotFoundError(f"{data} introuvable. Ex√©cute preprocessing.py")

    models_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../models"))
    os.makedirs(models_dir, exist_ok=True)  

    df = pd.read_csv(data)
    X = df.drop(columns=["target"])
    y = df["target"]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    model_file_path = os.path.join(models_dir, "model.pkl")

    joblib.dump(model, model_file_path)
    print(f"Mod√®le sauvegard√© : {model_file_path}")

```

---

## **3Ô∏è‚É£ √âtape 3 - √âvaluation du mod√®le**
**üìå Objectif :** Calculer la pr√©cision du mod√®le et sauvegarder le score.

üìç **Fichier : `src/evaluation.py`**
```python
import os
import pandas as pd
import joblib
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

def evaluate_model():
    print("√âvaluation du mod√®le...")

    data = os.path.abspath(os.path.join(os.path.dirname(__file__), "../data/iris.csv"))
    model = os.path.abspath(os.path.join(os.path.dirname(__file__), "../models/model.pkl"))

    # V√©rifier si les fichiers existent
    if not os.path.exists(data):
        raise FileNotFoundError(f"{data} introuvable. Ex√©cuter preprocessing.py")
    
    if not os.path.exists(model):
        raise FileNotFoundError(f"{model} introuvable. Ex√©cuter train_model.py")

    # Charger les donn√©es
    df = pd.read_csv(data)
    X = df.drop(columns=["target"])
    y = df["target"]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = joblib.load(model)

    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    print(f"Pr√©cision du mod√®le : {accuracy:.4f}")

    return accuracy

```

---

## **4Ô∏è‚É£ √âtape 4 - DAG Airflow**
üìç **Fichier : `dags/mlops_pipeline.py`**
```python
import logging
from datetime import datetime, timedelta
from airflow.models.dag import DAG
from airflow.operators.python import PythonOperator

from src.preprocessing import preprocess_data
from src.training import train_model
from src.evaluation import evaluate_model
from src.generate_report import generate_report

default_args = {
    'owner': 'mlops-airflow',
    'retries': 1,
    'retry_delay': timedelta(minutes=10),
    'start_date': datetime.now(),
}

dag = DAG('mlops_pipeline',
         default_args=default_args,
         schedule_interval=timedelta(days=1),
         catchup=False
)

task_1 = PythonOperator(
    task_id='preprocess_data',
    python_callable=preprocess_data,
    dag=dag
)

task_2 = PythonOperator(
    task_id='train_model',
    python_callable=train_model,
    dag=dag
)

task_3 = PythonOperator(
    task_id='evaluate_model',
    python_callable=evaluate_model,
    dag=dag
)

task_1 >> task_2 >> task_3
```

---

## **5Ô∏è‚É£ Lancer le pipeline**
1Ô∏è‚É£ **D√©marrer Airflow avec Docker**
```sh
docker-compose up
```

2Ô∏è‚É£ **Acc√©der √† Airflow**
- Ouvrir **http://localhost:8080**
- Activer et ex√©cuter le DAG `mlops_pipeline`

---

## Gestion des erreurs de modules dans Apache Airflow avec Docker

### üìå La m√©thode la plus simple
La fa√ßon la plus simple d‚Äôajouter vos d√©pendances est de modifier le fichier `docker-compose.yaml`. 

- **√âtape 1** : Rendez-vous √† la **ligne 71** du fichier `docker-compose.yaml`.
- **√âtape 2** : Ajoutez tous les modules Python dont vous avez besoin √† la variable `_PIP_ADDITIONAL_REQUIREMENTS`.

Cependant, pour une gestion plus propre et plus professionnelle des d√©pendances, il est recommand√© d‚Äôadopter une approche plus robuste en construisant une image Docker personnalis√©e.

üîó **Pour en savoir plus sur la bonne mani√®re de g√©rer les d√©pendances, consultez la documentation officielle d'Airflow :**  
[Airflow Docker Customization](https://airflow.apache.org/docs/docker-stack/build.html#example-of-adding-pypi-package)

---

### üöÄ Installation propre des d√©pendances avec Dockerfile

#### **√âtape 1 : Cr√©er un Dockerfile**
Afin de pouvoir installer les modules n√©cessaires via un fichier `requirements.txt`, commencez par cr√©er un `Dockerfile` dans le m√™me dossier que `docker-compose.yaml`.

Assurez-vous √©galement d‚Äôavoir un fichier `requirements.txt` listant vos d√©pendances.

---

### ‚ö†Ô∏è **Attention aux d√©pendances sp√©cifiques**
Certaines biblioth√®ques n√©cessitent des d√©pendances syst√®me sp√©cifiques. 
Par exemple, `scikit-learn` a besoin de `gcc`, `g++`, `make` et `python3-dev` pour √™tre compil√© correctement.

Ajoutez donc ces d√©pendances dans le `Dockerfile` avant d'installer `requirements.txt` :

```dockerfile
FROM apache/airflow:2.10.5

USER root

RUN apt-get update && apt-get install -y \
    gcc g++ make python3-dev

USER airflow

COPY requirements.txt /requirements.txt

RUN pip install --no-cache-dir "apache-airflow==${AIRFLOW_VERSION}" -r /requirements.txt

```

#### **√âtape 2 : Modifier le `docker-compose.yaml`**

Dans le fichier `docker-compose.yaml` :
- **Commentez** la ligne 52 :
  ```yaml
  # image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.10.5}
  ```
- **D√©commentez** la ligne 53 pour utiliser le Dockerfile que nous avons cr√©√© :
  ```yaml
  build: .
  ```

#### **√âtape 3 : Construire et lancer les conteneurs**
Ex√©cutez les commandes suivantes pour reconstruire et d√©marrer l‚Äôapplication :
```sh
docker-compose build
docker-compose up -d
```

Avec cette approche, votre environnement Airflow est bien configur√© et pr√™t √† ex√©cuter des t√¢ches avec toutes les d√©pendances requises. ‚úÖ

üéØ **F√©licitations !** Tu as maintenant un pipeline MLOps complet avec **Airflow** pour **entra√Æner, √©valuer et automatiser** un mod√®le de classification Iris ! üöÄ

## **6Ô∏è‚É£ G√©n√©ration du rapport**

Maintenant qu'on a `accuracy`, nous allons g√©n√©rer un rapport contenant cette m√©trique.

### **üìå Objectif : G√©n√©rer un rapport avec l'accuracy**

üìç **Fichier : `src/generate_report.py`**

```python
import os

def generate_report(**kwargs):
    ti = kwargs['ti']  # R√©cup√©rer l'accuracy depuis XCom
    accuracy = ti.xcom_pull(task_ids='evaluate_model', key='accuracy')
    
    if accuracy is None:
        raise ValueError("L'accuracy n'a pas √©t√© trouv√©e dans XCom.")
    
    report_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../report"))
    os.makedirs(report_dir, exist_ok=True)
    
    report_content = f"""
    # Rapport d'√âvaluation du Mod√®le

    **Accuracy**: {accuracy}
    
    **Commentaire**: Le mod√®le a atteint une accuracy de {accuracy}.
    """
    
    report_path = os.path.join(report_dir, "model_accuracy_report.md")
    with open(report_path, "w") as file:
        file.write(report_content)
    
    return report_path
```
Nous avons ajout√© la g√©n√©ration du rapport, nous devons mettre √† jour le **DAG** pour inclure cette nouvelle t√¢che.

üìç **Fichier : `mlops_pipeline.py`**
```python
from src.generate_report import generate_report

task_4 = PythonOperator(
    task_id='generate_report',
    python_callable=generate_report,
    provide_context=True,
    dag=dag
)

task_3 >> task_4
```

Pour s'assurer `evaluate_model`  envoie correctement l'accuracy et que `task_3` r√©cup√®re bien la valeur transmise, on doit faire certaine modification.

### **üìå Modification de `evaluate_model.py` pour envoyer correctement l'accuracy**

Dans **`src/evaluation.py`**, nous devons nous assurer que l'accuracy est bien stock√©e dans XCom et r√©cup√©rable par les t√¢ches suivantes.

üìç **Fichier : `src/evaluation.py`**
```python
import os
import pandas as pd
import joblib
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

def evaluate_model(**kwargs):
    print("√âvaluation du mod√®le...")

    data = os.path.abspath(os.path.join(os.path.dirname(__file__), "../data/iris.csv"))
    model = os.path.abspath(os.path.join(os.path.dirname(__file__), "../models/model.pkl"))

    # V√©rifier si les fichiers existent
    if not os.path.exists(data):
        raise FileNotFoundError(f"{data} introuvable. Ex√©cuter preprocessing.py")
    
    if not os.path.exists(model):
        raise FileNotFoundError(f"{model} introuvable. Ex√©cuter train_model.py")

    # Charger les donn√©es
    df = pd.read_csv(data)
    X = df.drop(columns=["target"])
    y = df["target"]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = joblib.load(model)

    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    print(f"Pr√©cision du mod√®le : {accuracy:.4f}")

    ti = kwargs['ti']  
    ti.xcom_push(key='accuracy', value=accuracy)

    return accuracy

```

### **üìå Modifier `task_3` dans `mlops_pipeline.py` pour recevoir les donn√©es correctement**

Dans **`mlops_pipeline.py`**, nous devons nous assurer que `task_3` (√©valuation) est bien configur√© pour recevoir et transmettre les donn√©es.

üìç **Fichier : `mlops_pipeline.py`**
```python
from src.evaluation import evaluate_model

task_3 = PythonOperator(
    task_id='evaluate_model',
    python_callable=evaluate_model,
    provide_context=True,  # ‚úÖ Assurer le passage des donn√©es via XCom
    dag=dag
)
```

Une fois ces modifications effectu√©es, `evaluate_model` enverra correctement l'accuracy, et `task_3` r√©cup√©rera et transmettra bien les donn√©es pour `generate_report`.


