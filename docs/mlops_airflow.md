# ğŸ“Œ Faisons un petit cas d'usage: Pipeline MLOps avec Airflow et Docker

## **Objectif**
L'objectif de cet exercice est de mettre en place un **pipeline MLOps automatisÃ©** avec **Apache Airflow** et **Docker** pour gÃ©rer l'entraÃ®nement et l'Ã©valuation d'un modÃ¨le de classification sur le dataset **Iris**.

Nous allons :
1. **Charger et prÃ©traiter les donnÃ©es** ğŸ“Š
2. **EntraÃ®ner un modÃ¨le RandomForest** ğŸ¯
3. **Ã‰valuer la performance du modÃ¨le** ğŸ“ˆ
4. **Mettre en place un DAG Airflow pour automatiser ces Ã©tapes**

---

## ğŸ“‚ **Structure du projet**

```plaintext
mlops/
â”‚â”€â”€ airflow-docker/
â”‚   â”œâ”€â”€ dags/                      # Contient les DAGs (workflow Airflow)
â”‚   â”‚   â”œâ”€â”€ mlops_pipeline.py
â”‚   â”œâ”€â”€ plugins/                   # Plugins Airflow si nÃ©cessaire
â”‚   â”œâ”€â”€ logs/                       # Logs d'exÃ©cution
â”‚   â”œâ”€â”€ docker-compose.yml          # DÃ©ploiement Docker
â”‚   â”œâ”€â”€ requirements.txt            # DÃ©pendances
â”‚â”€â”€ src/                            # Code mÃ©tier (prÃ©traitement, entraÃ®nement...)
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ training.py
â”‚   â”œâ”€â”€ evaluation.py
â”‚â”€â”€ data/                           # Datasets si besoin
â”‚   â”œâ”€â”€ data.csv
â”‚â”€â”€ models/                         # Stockage des modÃ¨les entraÃ®nÃ©s
â”‚â”€â”€ api/                            # API FastAPI pour servir le modÃ¨le
â”‚   â”œâ”€â”€ app.py
```

---

## **1ï¸âƒ£ Ã‰tape 1 - PrÃ©traitement des donnÃ©es**
**ğŸ“Œ Objectif :** Charger et nettoyer les donnÃ©es Iris, puis les sauvegarder.

ğŸ“ **Fichier : `src/preprocessing.py`**
```python
import pandas as pd
from sklearn.datasets import load_iris

def preprocess_data():
    print("PrÃ©traitement des donnÃ©es...")
    
    # Charger le dataset Iris
    iris = load_iris()
    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
    df['target'] = iris.target

    # Sauvegarder les donnÃ©es
    df.to_csv("data/iris.csv", index=False)
    print(" DonnÃ©es sauvegardÃ©es : data/iris.csv")

    return "data/iris.csv"
```

---

## **2ï¸âƒ£ Ã‰tape 2 - EntraÃ®nement du modÃ¨le**
**ğŸ“Œ Objectif :** EntraÃ®ner un modÃ¨le `RandomForestClassifier` et sauvegarder le modÃ¨le.

ğŸ“ **Fichier : `src/training.py`**
```python
import pandas as pd
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

def train_model():
    print(" EntraÃ®nement du modÃ¨le...")

    # Charger les donnÃ©es
    df = pd.read_csv("data/iris.csv")
    X = df.drop(columns=["target"])
    y = df["target"]

    # Split train/test
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # EntraÃ®ner le modÃ¨le
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    # Sauvegarder le modÃ¨le
    joblib.dump(model, "models/model.pkl")
    print("ModÃ¨le sauvegardÃ© : models/model.pkl")

    return "models/model.pkl"
```

---

## **3ï¸âƒ£ Ã‰tape 3 - Ã‰valuation du modÃ¨le**
**ğŸ“Œ Objectif :** Calculer la prÃ©cision du modÃ¨le et sauvegarder le score.

ğŸ“ **Fichier : `src/evaluation.py`**
```python
import pandas as pd
import joblib
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

def evaluate_model():
    print(" Ã‰valuation du modÃ¨le...")

    # Charger les donnÃ©es
    df = pd.read_csv("data/iris.csv")
    X = df.drop(columns=["target"])
    y = df["target"]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Charger le modÃ¨le
    model = joblib.load("models/model.pkl")
    y_pred = model.predict(X_test)

    # Calculer la prÃ©cision
    accuracy = accuracy_score(y_test, y_pred)
    print(f" PrÃ©cision du modÃ¨le : {accuracy:.4f}")

    return accuracy
```

---

## **4ï¸âƒ£ Ã‰tape 4 - DAG Airflow**
ğŸ“ **Fichier : `dags/mlops_pipeline.py`**
```python
import logging
from datetime import datetime, timedelta
from airflow.models.dag import DAG
from airflow.operators.python import PythonOperator
import sys
import os

# Ajoute src/ au PYTHONPATH
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../src')))

# Importation des fonctions
from preprocessing import preprocess_data
from training import train_model
from evaluation import evaluate_model

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 2, 23),
    'retries': 1,
}

with DAG('mlops_pipeline',
         default_args=default_args,
         schedule_interval='@daily',
         catchup=False) as dag:

    task_1 = PythonOperator(
        task_id='preprocess_data',
        python_callable=preprocess_data
    )

    task_2 = PythonOperator(
        task_id='train_model',
        python_callable=train_model
    )

    task_3 = PythonOperator(
        task_id='evaluate_model',
        python_callable=evaluate_model
    )

    task_1 >> task_2 >> task_3
```

---

## **5ï¸âƒ£ Lancer le pipeline**
1ï¸âƒ£ **DÃ©marrer Airflow avec Docker**
```sh
docker-compose up -d
```

2ï¸âƒ£ **AccÃ©der Ã  Airflow**
- Ouvrir **http://localhost:8080**
- Activer et exÃ©cuter le DAG `mlops_pipeline`

---

ğŸ¯ **FÃ©licitations !** Tu as maintenant un pipeline MLOps complet avec **Airflow** pour **entraÃ®ner, Ã©valuer et automatiser** un modÃ¨le de classification Iris ! ğŸš€
